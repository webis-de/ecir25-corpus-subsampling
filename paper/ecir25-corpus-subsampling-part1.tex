\section{Introduction}

\input{table-recommendet-subsamples}

We can also frame this as how big can you make the corpus post-hoc. E.g., like the "how much freedom paper". I.e., given a judgment pool, what is the legitimate/justified size to which you can grow the corpus? How much freedom does the judgment pool give us?

Unjudged documents are valuable, as they prevent overestimation of effectiveness in post hoc experiments and provide the basis for good corpora statistics. However, what is enough? E.g., I think (not sure about this) sharded implementations of BM25, e.g., elasticsearch do not use global term statistics, only per-shard ones.

Corpora: Robust04, TREC DL 19/20, ClueWeb09/12

Potentially apply it to BEIR as well.


Accessibility plays a key role in terms on which corpora people work. There are some corpora that have are substantially less robust compared to TREC corpora (e.g., some of the BEIR ones), still, people use those more, just because it is easier accessible, i.e., smaller. New corpora like the ClueWeb22 come with more open licenses then previous ones. So they trade off robustness for acessibility. Bring some examples like that, try to frame everything positively.

In other areas that deal with big data, refining, shaping, and making resources smaller is a common thing and highly valued by their respective communities. The same can be true for IR, we want to make huge experiments, but does this mean that everyone has to do everything all the time? The overall, big shared experiment should still remain valid, but can we save costs?
Bring the analogy to the fineweb? Or similar efforts? This is somewhat analogous. Analogous: wie viel vom web braucht man? Studie von Microsoft Research: kleiner corpus reicht f√ºr 80\% der queries. Wahrscheinlich war das gemeint~\cite{mei:2008}.


Unjudged documents are "valuable"
Prevent overestimation in post-hoc experiments
But: Unjudged documents introduce computational costs
Many modern approaches are expensive
Do we need to process all unjudged documents on for robust evaluations?




Unjudged documents are "valuable"
Prevent overestimation in post-hoc experiments
But: Unjudged documents introduce computational costs
Many modern approaches are expensive
Do we need to process all unjudged documents on for robust evaluations?
Example from last week: PLAID SHIRTTT
Processed 50% of the ClueWeb09 (0.5b English of 1b documents)
Took 71 days on an NVidia V100 GPU
Condensed list evaluation: treat unjudged documents as not retrieved
84366 of 1b documents judged for some query ( 0.01% of the corpus)
Two frequently applied scenarios:
Judgment pool only: you overestimate your effectiveness
Complete corpus: you underestimate your effectiveness
From the Green IR perspective: Can we find some trade-off in between?


Input:
set of pooled runs
judgment pool
Output:
Subset of the corpus
Potential subsamples:
Top-100 or 1000 pool as corpus maybe robust for a top-10 judgment pool?
Top-100 retrieval pool as corpus: <= 90\% documents unjudged
Top-1000 retrieval pool as corpus: <= 99\% documents unjudged
They likely cover diverse types of "hard negatives"



Table~\ref{table-recommendet-subsamples} provides our recommendet subsamples for all corpora xyz.
